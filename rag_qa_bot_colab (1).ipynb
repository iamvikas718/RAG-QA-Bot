{"cells":[{"cell_type":"code","execution_count":7,"id":"f7208b8d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"f7208b8d","executionInfo":{"status":"error","timestamp":1751779792789,"user_tz":-330,"elapsed":30814,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}},"outputId":"0bca08a3-2740-4ba2-f244-135ee05f62f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Business QA Bot is ready! Try asking a question:\n","Example questions:\n","- What is your return policy?\n","- How do I track my order?\n","- What payment methods do you accept?\n","\n","Your question (type 'quit' to exit): when will the package arrive\n"]},{"output_type":"error","ename":"RateLimitError","evalue":"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-7-1551796310.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nAnswer: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-7-1551796310.py\u001b[0m in \u001b[0;36mask_question\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Generate answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-7-1551796310.py\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(query, context)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Use the new chat completions API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use a chat model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         messages=[\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1086\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"]}],"source":["!pip install -q openai sentence-transformers\n","\n","\n","import openai\n","from sentence_transformers import SentenceTransformer\n","import time\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import google.generativeai as genai\n","from google.colab import userdata\n","\n","\n","OPENAI_API_KEY = userdata.get('OPEN_AI_KEY')\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","\n","openai.api_key = OPENAI_API_KEY\n","genai.configure(api_key=GOOGLE_API_KEY)\n","\n","\n","embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","business_faq = [\n","    {\"question\": \"What is your return policy?\", \"answer\": \"We accept returns within 30 days of purchase with original receipt.\"},\n","    {\"question\": \"How do I track my order?\", \"answer\": \"You can track your order using the tracking number in your confirmation email.\"},\n","    {\"question\": \"What payment methods do you accept?\", \"answer\": \"We accept Visa, Mastercard, American Express, and PayPal.\"},\n","    {\"question\": \"Do you offer international shipping?\", \"answer\": \"Yes, we ship to over 50 countries worldwide.\"},\n","    {\"question\": \"How can I contact customer service?\", \"answer\": \"Call us at 1-800-123-4567 or email support@example.com.\"}\n","]\n","\n","documents = [f\"Q: {item['question']} A: {item['answer']}\" for item in business_faq]\n","\n","def get_gemini_embeddings(texts):\n","    \"\"\"Generates Gemini embeddings for a list of texts.\"\"\"\n","    embeddings = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=texts,\n","        task_type=\"retrieval_document\"\n","    )\n","    return embeddings['embedding']\n","\n","document_embeddings = []\n","batch_size = 32\n","\n","for i in range(0, len(documents), batch_size):\n","    i_end = min(i + batch_size, len(documents))\n","    batch_texts = documents[i:i_end]\n","    batch_embeddings = get_gemini_embeddings(batch_texts)\n","    document_embeddings.extend(batch_embeddings)\n","\n","document_embeddings_np = np.array(document_embeddings)\n","\n","def find_similar_documents(query_embedding, document_embeddings_array, top_k=3):\n","    \"\"\"Finds the top_k most similar documents to a query embedding.\"\"\"\n","    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n","    similarities = cosine_similarity(query_embedding_np, document_embeddings_array)\n","    top_k_indices = np.argsort(similarities[0])[-top_k:][::-1]\n","    return top_k_indices\n","\n","def retrieve(query, top_k=3):\n","    \"\"\"Retrieves top_k most similar documents to a query using Gemini embeddings.\"\"\"\n","    query_embedding = get_gemini_embeddings(query)\n","    top_k_indices = find_similar_documents(query_embedding, document_embeddings_np, top_k=top_k)\n","\n","\n","    retrieved_documents = [documents[i] for i in top_k_indices]\n","    return retrieved_documents\n","\n","\n","def generate_answer(query, context):\n","    prompt = f\"\"\"Answer the question based on the context below. If you don't know the answer, say \"I don't know\".\n","\n","Context:\n","{context}\n","\n","Question: {query}\n","Answer:\"\"\"\n","\n","    response = openai.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        temperature=0,\n","        max_tokens=150\n","    )\n","\n","    return response.choices[0].message.content.strip()\n","\n","def ask_question(question):\n","    retrieved = retrieve(question)\n","\n","\n","    context = \"\\n\\n\".join(retrieved)\n","\n","\n","    answer = generate_answer(question, context)\n","\n","    return answer\n","\n","print(\"\\nBusiness QA Bot is ready! Try asking a question:\")\n","print(\"Example questions:\")\n","print(\"- What is your return policy?\")\n","print(\"- How do I track my order?\")\n","print(\"- What payment methods do you accept?\")\n","\n","while True:\n","    user_input = input(\"\\nYour question (type 'quit' to exit): \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = ask_question(user_input)\n","    print(f\"\\nAnswer: {response}\")\n"]},{"cell_type":"code","metadata":{"id":"dd4154d8","executionInfo":{"status":"aborted","timestamp":1751779435923,"user_tz":-330,"elapsed":28456,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["!pip install --upgrade datasets"],"id":"dd4154d8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"beb3fa76"},"source":["# Task\n","Replace the Pinecone vector database implementation in the provided Python code with a local similarity search using Gemini embeddings. Use the Google API key stored in the \"GOOGLE_API_KEY\" file for the Gemini embeddings. The code should still perform a RAG (Retrieval Augmented Generation) task, where relevant documents are retrieved based on a query and then used as context to generate an answer using OpenAI."],"id":"beb3fa76"},{"cell_type":"markdown","metadata":{"id":"79fbc461"},"source":["## Install necessary libraries\n","\n","### Subtask:\n","Ensure the Google Generative AI library is installed.\n"],"id":"79fbc461"},{"cell_type":"markdown","metadata":{"id":"20d75ed5"},"source":["**Reasoning**:\n","The subtask is to install the `google-generativeai` library. This can be done using pip in a code block.\n","\n"],"id":"20d75ed5"},{"cell_type":"code","metadata":{"id":"cbcd7f71","executionInfo":{"status":"aborted","timestamp":1751779435927,"user_tz":-330,"elapsed":2,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["!pip install google-generativeai"],"id":"cbcd7f71","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ee59bb0c"},"source":["## Initialize gemini embeddings\n","\n","### Subtask:\n","Set up the Gemini embeddings model using your Google API key.\n"],"id":"ee59bb0c"},{"cell_type":"markdown","metadata":{"id":"11b69e78"},"source":["**Reasoning**:\n","Set up the Gemini embeddings model using your Google API key by importing the necessary library, getting the API key from secrets, and configuring the library.\n","\n"],"id":"11b69e78"},{"cell_type":"code","metadata":{"id":"799955f1","executionInfo":{"status":"aborted","timestamp":1751779435930,"user_tz":-330,"elapsed":28455,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["import google.generativeai as genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","genai.configure(api_key=GOOGLE_API_KEY)"],"id":"799955f1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37dfe2f0"},"source":["## Embed documents\n","\n","### Subtask:\n","Generate embeddings for the documents using the Gemini embeddings model.\n"],"id":"37dfe2f0"},{"cell_type":"markdown","metadata":{"id":"685292dd"},"source":["**Reasoning**:\n","Define a function to get Gemini embeddings and then iterate through the documents in batches to generate and store the embeddings.\n","\n"],"id":"685292dd"},{"cell_type":"code","metadata":{"id":"5534b3e8","executionInfo":{"status":"aborted","timestamp":1751779435933,"user_tz":-330,"elapsed":28455,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def get_gemini_embeddings(texts):\n","    \"\"\"Generates Gemini embeddings for a list of texts.\"\"\"\n","    embeddings = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=texts,\n","        task_type=\"retrieval_query\" # Or \"retrieval_document\" depending on usage later\n","    )\n","    return [item['embedding'] for item in embeddings['embeddings']]\n","\n","document_embeddings = []\n","batch_size = 32 # Reuse the batch_size from the previous code cell\n","\n","for i in range(0, len(documents), batch_size):\n","    i_end = min(i + batch_size, len(documents))\n","    batch_texts = documents[i:i_end]\n","    batch_embeddings = get_gemini_embeddings(batch_texts)\n","    document_embeddings.extend(batch_embeddings)\n","\n","print(f\"Generated embeddings for {len(document_embeddings)} documents\")"],"id":"5534b3e8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1c634336"},"source":["**Reasoning**:\n","The previous code failed because the structure of the `embeddings` object returned by `genai.embed_content` was not as expected. I need to inspect the structure of the returned object to correctly extract the embeddings.\n","\n"],"id":"1c634336"},{"cell_type":"code","metadata":{"id":"SF0gyvHvuoho","executionInfo":{"status":"aborted","timestamp":1751779435937,"user_tz":-330,"elapsed":28456,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def get_gemini_embeddings(texts):\n","    \"\"\"Generates Gemini embeddings for a list of texts.\"\"\"\n","    embeddings = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=texts,\n","        task_type=\"retrieval_document\" # Changed to retrieval_document for document embedding\n","    )\n","    # Inspect the structure of the embeddings object\n","    print(embeddings)\n","    return [item['embedding'] for item in embeddings['embeddings']]\n","\n","# Temporarily test with a small batch to inspect the output\n","test_batch = documents[:2]\n","test_embeddings_output = get_gemini_embeddings(test_batch)\n","\n","# Continue with the rest of the code after fixing the extraction\n","document_embeddings = []\n","batch_size = 32\n","\n","for i in range(0, len(documents), batch_size):\n","    i_end = min(i + batch_size, len(documents))\n","    batch_texts = documents[i:i_end]\n","    # Assuming the structure is now understood and the extraction logic is correct\n","    batch_embeddings = get_gemini_embeddings(batch_texts) # This call will still print the structure\n","    document_embeddings.extend(batch_embeddings)\n","\n","print(f\"Generated embeddings for {len(document_embeddings)} documents\")"],"id":"SF0gyvHvuoho","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"010e8f70"},"source":["**Reasoning**:\n","The print output shows that the embedding is directly under the key 'embedding', not nested under 'embeddings'. I need to correct the extraction logic in the `get_gemini_embeddings` function.\n","\n"],"id":"010e8f70"},{"cell_type":"code","metadata":{"id":"BNMEzQOyurNZ","executionInfo":{"status":"aborted","timestamp":1751779435959,"user_tz":-330,"elapsed":28474,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def get_gemini_embeddings(texts):\n","    \"\"\"Generates Gemini embeddings for a list of texts.\"\"\"\n","    embeddings = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=texts,\n","        task_type=\"retrieval_document\"\n","    )\n","    # Corrected extraction: Access the 'embedding' key directly\n","    return embeddings['embedding']\n","\n","document_embeddings = []\n","batch_size = 32\n","\n","for i in range(0, len(documents), batch_size):\n","    i_end = min(i + batch_size, len(documents))\n","    batch_texts = documents[i:i_end]\n","    batch_embeddings = get_gemini_embeddings(batch_texts)\n","    document_embeddings.extend(batch_embeddings)\n","\n","print(f\"Generated embeddings for {len(document_embeddings)} documents\")"],"id":"BNMEzQOyurNZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4744344c"},"source":["## Implement similarity search\n","\n","### Subtask:\n","Replace the Pinecone index search with a method to perform similarity search on the Gemini embeddings. This could involve using libraries like scikit-learn or Faiss for efficient search.\n"],"id":"4744344c"},{"cell_type":"markdown","metadata":{"id":"1897b5a9"},"source":["**Reasoning**:\n","Import cosine_similarity and numpy, convert document_embeddings to a numpy array, and define the find_similar_documents function to calculate cosine similarity and return top_k indices.\n","\n"],"id":"1897b5a9"},{"cell_type":"code","metadata":{"id":"19282704","executionInfo":{"status":"aborted","timestamp":1751779435962,"user_tz":-330,"elapsed":28473,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","document_embeddings_np = np.array(document_embeddings)\n","\n","def find_similar_documents(query_embedding, document_embeddings_array, top_k=3):\n","    \"\"\"Finds the top_k most similar documents to a query embedding.\"\"\"\n","    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n","    similarities = cosine_similarity(query_embedding_np, document_embeddings_array)\n","    # Get indices of top_k similar documents\n","    top_k_indices = np.argsort(similarities[0])[-top_k:][::-1]\n","    return top_k_indices"],"id":"19282704","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9b84be3"},"source":["## Update retrieval function\n","\n","### Subtask:\n","Modify the `retrieve` function to use the new similarity search method to find relevant documents based on the query embedding.\n"],"id":"a9b84be3"},{"cell_type":"markdown","metadata":{"id":"6b76397d"},"source":["**Reasoning**:\n","Implement the `retrieve` function to use the local similarity search method with Gemini embeddings.\n","\n"],"id":"6b76397d"},{"cell_type":"code","metadata":{"id":"f4dfe70d","executionInfo":{"status":"aborted","timestamp":1751779435975,"user_tz":-330,"elapsed":28483,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def retrieve(query, top_k=3):\n","    \"\"\"Retrieves top_k most similar documents to a query using Gemini embeddings.\"\"\"\n","    query_embedding = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=query,\n","        task_type=\"retrieval_query\"\n","    )['embedding'] # Access the 'embedding' key directly\n","\n","    top_k_indices = find_similar_documents(query_embedding, document_embeddings_np, top_k=top_k)\n","\n","    # Retrieve the actual text of the top_k documents\n","    retrieved_documents = [documents[i] for i in top_k_indices]\n","    return retrieved_documents\n","\n","print(\"retrieve function updated to use local similarity search.\")"],"id":"f4dfe70d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"409a29f2"},"source":["## Update rag query function\n","\n","### Subtask:\n","Modify the `rag_query` function to use the updated `retrieve` function.\n"],"id":"409a29f2"},{"cell_type":"markdown","metadata":{"id":"b567ee26"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   The `google-generativeai` library was confirmed to be already installed, allowing for the use of Gemini embeddings.\n","*   The Gemini embeddings model was successfully initialized using the provided Google API key.\n","*   A function was created to generate Gemini embeddings for document texts in batches, correcting an initial issue with accessing the embedding data.\n","*   A local similarity search method was implemented using cosine similarity from the scikit-learn library.\n","*   The `retrieve` function was updated to utilize the local similarity search with Gemini embeddings for query retrieval.\n","*   No changes were required for the `rag_query` function as it was already compatible with the updated `retrieve` function.\n","\n","### Insights or Next Steps\n","\n","*   The implementation successfully replaced the external Pinecone vector database with a local similarity search using Gemini embeddings, providing a self-contained RAG system.\n","*   For larger datasets, consider implementing more efficient local search methods like Faiss to improve retrieval performance.\n"],"id":"b567ee26"},{"cell_type":"markdown","metadata":{"id":"200241d3"},"source":["## Install necessary libraries\n","\n","### Subtask:\n","Ensure the Google Generative AI library is installed."],"id":"200241d3"},{"cell_type":"markdown","metadata":{"id":"0d2b63c1"},"source":["**Reasoning**:\n","The subtask is to install the `google-generativeai` library. This can be done using pip in a code block."],"id":"0d2b63c1"},{"cell_type":"code","metadata":{"id":"93363d61","executionInfo":{"status":"aborted","timestamp":1751779435987,"user_tz":-330,"elapsed":28492,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["!pip install google-generativeai"],"id":"93363d61","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4d779a42"},"source":["## Initialize gemini embeddings\n","\n","### Subtask:\n","Set up the Gemini embeddings model using your Google API key."],"id":"4d779a42"},{"cell_type":"markdown","metadata":{"id":"31d4d570"},"source":["**Reasoning**:\n","Set up the Gemini embeddings model using your Google API key by importing the necessary library, getting the API key from secrets, and configuring the library."],"id":"31d4d570"},{"cell_type":"code","metadata":{"id":"9abf525d","executionInfo":{"status":"aborted","timestamp":1751779435989,"user_tz":-330,"elapsed":28491,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["import google.generativeai as genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","genai.configure(api_key=GOOGLE_API_KEY)"],"id":"9abf525d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0b80be31"},"source":["## Embed documents\n","\n","### Subtask:\n","Generate embeddings for the documents using the Gemini embeddings model."],"id":"0b80be31"},{"cell_type":"markdown","metadata":{"id":"bc78cc76"},"source":["**Reasoning**:\n","Define a function to get Gemini embeddings and then iterate through the documents in batches to generate and store the embeddings."],"id":"bc78cc76"},{"cell_type":"code","metadata":{"id":"2166ee02","executionInfo":{"status":"aborted","timestamp":1751779435991,"user_tz":-330,"elapsed":28491,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def get_gemini_embeddings(texts):\n","    \"\"\"Generates Gemini embeddings for a list of texts.\"\"\"\n","    embeddings = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=texts,\n","        task_type=\"retrieval_document\" # Changed to retrieval_document for document embedding\n","    )\n","    # Corrected extraction: Access the 'embedding' key directly\n","    return embeddings['embedding']\n","\n","document_embeddings = []\n","batch_size = 32 # Reuse the batch_size from the previous code cell\n","\n","for i in range(0, len(documents), batch_size):\n","    i_end = min(i + batch_size, len(documents))\n","    batch_texts = documents[i:i_end]\n","    batch_embeddings = get_gemini_embeddings(batch_texts)\n","    document_embeddings.extend(batch_embeddings)\n","\n","print(f\"Generated embeddings for {len(document_embeddings)} documents\")"],"id":"2166ee02","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15e9ff26"},"source":["## Implement similarity search\n","\n","### Subtask:\n","Replace the Pinecone index search with a method to perform similarity search on the Gemini embeddings. This could involve using libraries like scikit-learn or Faiss for efficient search."],"id":"15e9ff26"},{"cell_type":"markdown","metadata":{"id":"ad3783d6"},"source":["**Reasoning**:\n","Import cosine_similarity and numpy, convert document_embeddings to a numpy array, and define the find_similar_documents function to calculate cosine similarity and return top_k indices."],"id":"ad3783d6"},{"cell_type":"code","metadata":{"id":"707f7a9f","executionInfo":{"status":"aborted","timestamp":1751779436004,"user_tz":-330,"elapsed":28501,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","document_embeddings_np = np.array(document_embeddings)\n","\n","def find_similar_documents(query_embedding, document_embeddings_array, top_k=3):\n","    \"\"\"Finds the top_k most similar documents to a query embedding.\"\"\"\n","    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n","    similarities = cosine_similarity(query_embedding_np, document_embeddings_array)\n","    # Get indices of top_k similar documents\n","    top_k_indices = np.argsort(similarities[0])[-top_k:][::-1]\n","    return top_k_indices"],"id":"707f7a9f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46734d92"},"source":["## Update retrieval function\n","\n","### Subtask:\n","Modify the `retrieve` function to use the new similarity search method to find relevant documents based on the query embedding."],"id":"46734d92"},{"cell_type":"markdown","metadata":{"id":"0dbb5a38"},"source":["**Reasoning**:\n","Implement the `retrieve` function to use the local similarity search method with Gemini embeddings."],"id":"0dbb5a38"},{"cell_type":"code","metadata":{"id":"607c40df","executionInfo":{"status":"aborted","timestamp":1751779436006,"user_tz":-330,"elapsed":28500,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def retrieve(query, top_k=3):\n","    \"\"\"Retrieves top_k most similar documents to a query using Gemini embeddings.\"\"\"\n","    query_embedding = genai.embed_content(\n","        model=\"models/embedding-001\",\n","        content=query,\n","        task_type=\"retrieval_query\"\n","    )['embedding'] # Access the 'embedding' key directly\n","\n","    top_k_indices = find_similar_documents(query_embedding, document_embeddings_np, top_k=top_k)\n","\n","    # Retrieve the actual text of the top_k documents\n","    retrieved_documents = [documents[i] for i in top_k_indices]\n","    return retrieved_documents\n","\n","print(\"retrieve function updated to use local similarity search.\")"],"id":"607c40df","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba874d40"},"source":["## Update rag query function\n","\n","### Subtask:\n","Modify the `rag_query` function to use the updated `retrieve` function."],"id":"ba874d40"},{"cell_type":"markdown","metadata":{"id":"9bd46be8"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","* The `google-generativeai` library was confirmed to be already installed, allowing for the use of Gemini embeddings.\n","* The Gemini embeddings model was successfully initialized using the provided Google API key.\n","* A function was created to generate Gemini embeddings for document texts in batches, correcting an initial issue with accessing the embedding data.\n","* A local similarity search method was implemented using cosine similarity from the scikit-learn library.\n","* The `retrieve` function was updated to utilize the local similarity search with Gemini embeddings for query retrieval.\n","* No changes were required for the `rag_query` function as it was already compatible with the updated `retrieve` function.\n","\n","### Insights or Next Steps\n","\n","* The implementation successfully replaced the external Pinecone vector database with a local similarity search using Gemini embeddings, providing a self-contained RAG system.\n","* For larger datasets, consider implementing more efficient local search methods like Faiss to improve retrieval performance."],"id":"9bd46be8"},{"cell_type":"code","metadata":{"id":"b8847aff","executionInfo":{"status":"aborted","timestamp":1751779436008,"user_tz":-330,"elapsed":28499,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["for m in genai.list_models():\n","  if 'generateContent' in m.supported_generation_methods:\n","    print(m.name)"],"id":"b8847aff","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bc4d8f5","executionInfo":{"status":"aborted","timestamp":1751779436009,"user_tz":-330,"elapsed":28497,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["# Define a sample query\n","sample_query = \"What is the capital of France?\" # Or any other query you want to test\n","\n","# Retrieve documents for the sample query\n","retrieved_docs = retrieve(sample_query)\n","\n","print(\"Retrieved Documents:\")\n","for i, doc in enumerate(retrieved_docs):\n","    print(f\"Document {i+1}:\\n{doc}\\n---\")\n","\n","# Generate the prompt that will be sent to the Gemini model\n","context_for_prompt = \"\\n\\n\".join(retrieved_docs)\n","prompt_for_gemini = f\"\"\"Answer the question based on the context below. If you don't know the answer, say \\\"I don't know\\\".\n","\n","Context:\n","{context_for_prompt}\n","\n","Question: {sample_query}\n","Answer:\"\"\"\n","\n","print(\"\\nPrompt sent to Gemini:\")\n","print(prompt_for_gemini)\n","\n","# You can then manually check if the retrieved documents contain the answer and if the prompt is correctly formatted.\n","# Running the full rag_query(sample_query) again would show the final answer.\n","# final_answer = rag_query(sample_query)\n","# print(f\"\\nFinal Answer from RAG: {final_answer}\")"],"id":"2bc4d8f5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88e81036","executionInfo":{"status":"aborted","timestamp":1751779436012,"user_tz":-330,"elapsed":28497,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["# Display the first few documents to understand the dataset content\n","print(\"Sample Documents from the dataset:\")\n","for i, doc in enumerate(documents[:5]): # Displaying the first 5 documents\n","    print(f\"Document {i+1}:\\n{doc}\\n---\")"],"id":"88e81036","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8e0f57a","executionInfo":{"status":"aborted","timestamp":1751779436013,"user_tz":-330,"elapsed":28496,"user":{"displayName":"N Vikas Reddy","userId":"09895925302380588524"}}},"source":["def retrieve(query, top_k=3):\n","    \"\"\"Retrieves top_k most similar documents to a query using Gemini embeddings.\"\"\"\n","    query_embedding = get_gemini_embeddings(query) # Use the corrected function\n","    top_k_indices = find_similar_documents(query_embedding, document_embeddings_np, top_k=top_k)\n","\n","    # Retrieve the actual text of the top_k documents\n","    retrieved_documents = [documents[i] for i in top_k_indices]\n","    return retrieved_documents\n","\n","print(\"retrieve function defined.\")"],"id":"e8e0f57a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1405b2ff"},"source":["Now that the `retrieve` function is defined, you can run the cells sequentially to use the RAG QA Bot."],"id":"1405b2ff"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}